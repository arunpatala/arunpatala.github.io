---
layout: post
title:  "Retrieval Augmented Generation with OPENAI"
date:   2024-02-07 11:53:18 +0530
categories: AI
---

## Table of Contents
1. [RAG (Retrieval Augmented Generation)](#rag-retrieval-augmented-generation)
2. [Example Dataset](#example-dataset)
3. [Setting Up the OpenAI API](#setting-up-the-openai-api)
5. [Embeddings](#embeddings)
4. [Approach](#approach)
6. [Steps](#steps)
7. [Conclusion](#conclusion)
8. [References](#references)

RAG (Retrieval Augmented Generation)
--------------------------------------------------------

Language Models (LLMs) have significantly advanced the field of question answering, offering insights and responses across a wide array of topics. However, despite their prowess, LLMs are not without their limitations—they can sometimes produce answers based on incorrect assumptions or "hallucinations," and their capabilities are bounded by the data they were trained on, leaving them unable to directly interact with or retrieve information from external databases in real-time.

Retrieval Augmented Generation (RAG) presents an innovative solution to these challenges. RAG is a process that enhances the capabilities of LLMs by retrieving relevant information from an external database, such as Wikipedia, in response to a query. This approach not only enables the model to access and incorporate new information it hasn't been explicitly trained on but also helps mitigate the issue of hallucinations. By providing contextually pertinent information to the LLM along with the query, RAG ensures that the generated answers are both informed and accurate, leveraging the retrieved data to augment the generation process. This synergy between retrieval and generation paves the way for more precise and reliable question answering, expanding the horizons of what LLMs can achieve.



Example Dataset
--------------------------------------------------------
To illustrate the application of the Retrieval Augmented Generation (RAG) process using the OpenAI API, let's consider solving a simple RAG problem centered around high school-level biology questions.

**Example Question:**

"What is the most distinctive feature of echinoderms?"

**Choices:**

A. Presence of an endoskeleton of calcareous ossicles  
B. Absence of an excretory system  
C. Presence of a water vascular system  
D. All of the above

In this scenario, we're provided with the text from biology chapters relevant to these questions. While it's possible to submit the entire chapter text alongside the question to a Large Language Model (LLM) for answering, the sheer size of the text might exceed the LLM's processing capacity. To address this, we break down the chapter text into manageable paragraphs. Our goal then becomes to retrieve the most relevant paragraphs that contain the information necessary to answer the question.

It's important to note that while our example involves a straightforward question likely answerable with just a few sentences, the RAG approach can also be adapted for more complex inquiries, such as those requiring summary and analysis. This demonstrates the versatility of RAG in handling a range of question complexities by identifying and utilizing the most pertinent information from a larger dataset.

The code and dataset can be found [here](https://github.com/arunpatala/arunpatala.github.io/tree/main/rag_openai).

Setting Up the OpenAI API
--------------------------------------------------------

To utilize the OpenAI API for our example, follow these concise steps:

1. **API Key:** Sign up at OpenAI and generate an API key. Store this key in an environment variable for security.
2. **Python Environment:** Ensure Python is installed. Use a virtual environment for dependency management.
3. **Installation:** Install the OpenAI library in your environment using `pip install openai`.

For detailed setup instructions, including how to securely handle your API key and manage your Python environment, please refer to the official OpenAI documentation: [OpenAI API Quickstart](https://platform.openai.com/docs/quickstart?context=python).



Embeddings
--------------------------------------------------------

Embeddings transform text into high-dimensional vectors, enabling texts with similar meanings to have closely aligned vectors. This similarity is often measured using cosine similarity, where a higher value indicates greater textual similarity. Generated through architectures like Large Language Models (LLMs) or BERT, embeddings capture complex linguistic features, allowing for effective semantic analysis. Recent advancements, such as contrastive embeddings, improve the accuracy of these representations by training models to distinguish more clearly between similar and dissimilar texts, enhancing tasks like information retrieval and question answering. For an in-depth look at contrastive embeddings, see [this paper](https://arxiv.org/abs/2201.10005), which explores their development and application.


Approach
--------------------------------------------------------

Our method for retrieving the most relevant chapter sections in response to a question involves the following steps:

1. **Query Embedding:** Generate an embedding for the question (query) using OpenAI's embedding capabilities.
2. **Section Embeddings:** Compute embeddings for each section of the chapter to represent their content numerically.
3. **Retrieval:** Utilize these embeddings to identify and retrieve the sections most similar to the query. This is achieved by measuring the similarity between the query embedding and each section embedding.
4. **Concatenation:** Combine the top 3 most similar sections with the original query. This aggregated content serves as the augmented input for the Large Language Model (LLM).
5. **Answer Generation:** Submit this concatenated input to the LLM, prompting it to generate an answer based on the provided context. If necessary, ask for citations to support the answer.

This process leverages the semantic understanding capabilities of embeddings to efficiently identify the most relevant information within a larger text, enabling the LLM to produce more accurate and informed responses. The following code and dataset can be found [here](https://github.com/arunpatala/arunpatala.github.io/tree/main/rag_openai).



Steps
--------------------------------------------------------

### 1. Reading Data and Creating Text

This subsection focuses on the first crucial step in the data preparation process for the Retrieval Augmented Generation (RAG) system: reading biology sample problems from a JSON file and converting them into a text format suitable for analysis. The process is implemented using Python and involves the following code snippets and explanations:

```python
import json

# Function to read data from a JSON file
def read_json(file_path):
    with open(file_path, 'r') as f:
        data = json.load(f)
    return data

# Specify the path to the bio_sample_problems.json file
file_path = 'bio_sample_problems.json'
data = read_json(file_path)

# Function to convert sections of a chapter into text
def convert_sections_to_text(sections):
    text = ""
    for section in sections:
        text += f"Section: {section['section']}, Title: {section['title']}\n"
    return text
```

The provided Python code demonstrates the initial step in preparing a dataset for the Retrieval Augmented Generation (RAG) process, focusing on reading and structuring data from a JSON file named `bio_sample_problems.json`. It begins by importing the JSON library and defining a function, `read_json`, to load the JSON content into a Python dictionary. Another function, `convert_sections_to_text`, is then used to format each section within the chapters into a readable string format, emphasizing the section's identifier and title. Finally, the code iterates over the chapters, transforming their sections into structured text and concatenating this with the chapter's main text. This process results in a collection of texts, each representing a chapter's content, formatted and ready for further processing and analysis in the RAG system.

This methodical approach to data preparation ensures that the dataset is optimally structured for the embedding and retrieval stages of the RAG process, facilitating accurate and contextually relevant responses to queries.


### 2. Calculating Text Embeddings

In this step, we transform the structured text from our dataset into numerical vectors, known as embeddings, using the OpenAI API. This process involves iterating over each text segment—paragraph or section—and converting it into a dense vector representation. To achieve this, we clean each text segment by removing newline characters to ensure consistency and then utilize the OpenAI API's embeddings.create function, specifying the text-embedding-3-small model. This model generates a high-dimensional vector for each text segment, capturing its semantic essence. These vectors are then compiled into an array, providing a numerical foundation for the next stage of our RAG process: identifying the most relevant text segments for a given query. The use of embeddings is crucial for enabling efficient and semantically aware retrieval of information.



```python
from openai import OpenAI
from tqdm import tqdm
import numpy as np
import os

client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

# Create embeddings for the text
embeddings = []
for text in tqdm(texts):
    text = text.replace("\n", " ")
    embedding = client.embeddings.create(input=[text], model='text-embedding-3-small').data[0].embedding
    embeddings.append(embedding)

embeddings = np.array(embeddings)
```

### 3. Calculating Query Embedding

To match the query with the most relevant information in our dataset, we first convert the query text into an embedding using the same OpenAI API method as for the dataset texts. By cleaning the query text of newline characters and generating its numerical vector representation with the `text-embedding-3-small` model, we ensure the query is in a format suitable for comparison. This step produces a dense vector (embedding) that encapsulates the query's semantic content, allowing for an effective similarity measurement with the dataset embeddings. The embedding is then formatted as a NumPy array, ready for the matching process.

```python
# Code snippet for calculating query embedding
problem_texts = [t.replace("\n", " ") for t in problem_texts]
pembedding = client.embeddings.create(input=[problem_texts[idx]], model='text-embedding-3-small').data[0].embedding
pembedding = np.array([pembedding])
```

This concise approach highlights the process of transforming the query into a comparable numerical format, pivotal for identifying relevant content within the dataset.

### 4. Identifying Top 3 Most Similar Sections

To pinpoint the sections most relevant to our query, we employ cosine similarity—a measure of similarity between two non-zero vectors. By calculating the cosine similarity between the query embedding and each section embedding in our dataset, we can assess how closely related each section is to the query. This process involves:

1. **Cosine Similarity Calculation:** A custom `cosine_similarity` function computes the similarity between the query embedding and each section's embedding, producing a similarity score for each.

2. **Ranking and Selection:** The similarity scores are then ranked, and the top 3 scores are selected. These scores correspond to the sections most closely related to the query, providing the context necessary for generating an accurate answer.

3. **Context Preparation:** The texts of these top 3 sections are concatenated, forming a comprehensive context. This context, along with the original query, is prepared for submission to the LLM for answer generation.

```python
# Code snippet for identifying top 3 most similar sections
similarities = [cosine_similarity(embedding, pembedding[0]) for embedding in embeddings]
top_indices = np.argsort(similarities)[::-1][:3]
context = "\n\n".join(texts[i] for i in top_indices)
context_plus_problem = context + problem_texts[idx]
```

This streamlined approach ensures that the most semantically relevant sections are selected to augment the LLM's response, enhancing the accuracy and relevance of the generated answers.

### 5. Generating the Answer with Contextual Information

The final step in our RAG process involves submitting the concatenated sections along with the question to an LLM (in this case, GPT-4) to generate a detailed answer. This is facilitated by crafting a prompt that instructs the model to act as a biology teacher, providing not only the answer to the multiple-choice question but also citing specific sentences from the provided information and explaining the answer in detail. The `call_data` function orchestrates this interaction, sending the combined context and question as input and specifying the model to use. 

```python
# Simplified code snippet for generating an answer with LLM
answer = call_data("", context_plus_problem + prompt)
```

This approach leverages the LLM's understanding of the context and its ability to synthesize information, resulting in a response that includes the correct answer choice, citation from the context validating the answer, and a comprehensive explanation. This method not only enhances the reliability of the answer but also provides valuable insights into the reasoning behind it, mimicking a personalized educational experience.

This concise explanation outlines the process of integrating contextual information with a user's query to produce informed and accurate answers, showcasing the power of combining retrieval-augmented generation with advanced language models.


Conclusion
--------------------------------------------------------

Implementing Retrieval Augmented Generation (RAG) with OpenAI's API marks a significant leap forward in enhancing the accuracy and depth of question-answering systems. This methodology, which integrates data preparation, embedding generation, and information retrieval, showcases how leveraging the latest advancements in AI can significantly improve the responses generated by large language models (LLMs). By grounding these responses in relevant, specific information, the RAG process not only enhances the quality of answers but also underscores the potential of AI in education and information retrieval, offering a pathway to more sophisticated, reliable, and personalized AI-driven tools.

References
--------------------------------------------------------

1. **RAG blogpost:** Detailed explanation of RAG architecture [blogpost](https://arunpatala.github.io/ai/2024/02/03/RAG.html).

1. **OpenAI API Documentation:** Provides foundational knowledge for using OpenAI's API, detailing setup, library installations, and basic interaction commands. [OpenAI API Quickstart](https://platform.openai.com/docs/quickstart?context=python).

2. **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding:** Introduces BERT, significantly improving language understanding efficiency across various tasks. [BERT Paper](https://arxiv.org/abs/1810.04805).

3. **Contrastive Learning of General-Purpose Audio Representations:** Offers insights into contrastive learning principles, applicable to enhancing text embeddings for RAG systems. [Contrastive Learning Paper](https://arxiv.org/abs/2201.10005).
