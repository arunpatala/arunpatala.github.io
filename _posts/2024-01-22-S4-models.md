---
layout: post
title:  "Structured State Space Models for Sequences (S4)"
date:   2024-01-22 10:53:18 +0530
categories: AI
---

Sequence modeling is a technique used to predict future data points based on the observed past data points. 
This approach is applicable to various types of data, such as stock prices, speech, text, and images, which can all be understood as sequences. 
Essentially, sequence modeling can be used to forecast an output (y) over time (t), based on a corresponding input (x) or the past (y) observed over time. 
For example, this can be seen in scenarios like predicting the trajectory of a missile using Kalman filters, based on its velocity data over time.

In sequence modeling, particularly with time series data, we often distinguish between continuous and discrete models.
Ideally, we aim to understand the continuous underlying function of a phenomenon, like the trajectory of a missile. 
However, in practice, we usually have data sampled at discrete time intervals, and these intervals can potentially vary. 
For example, a speech waveform is a continuous signal showing variations in air pressure over time, but we typically sample 
this waveform at consistent time intervals to create a discrete representation of the speech.

State Space Models: An Introduction to Sequence Modeling
--------------------------------------------------------

In the realm of sequence modeling, we often aim to establish a relationship between two continuous functions: an input function $$ x $$ and an output function $$ y $$. For simplicity, let's consider these functions to be one-dimensional and focus on real values. While we can explore discrete versions later, our initial discussion will revolve around continuous models.

To understand state space models, imagine the input as a continuous function $$ u(t) $$, and the output as another continuous function $$ y(t) $$, both of which are one-dimensional. In this context, the output $$ y $$ at any given time $$ t $$ is influenced by the input function up to that point in time.

State space models are built on a foundational assumption: there exists an internal, N-dimensional state $$ x(t) $$ that evolves over time. This evolution is influenced by both the current input function $$ u(t) $$ and the previous state of $$ x $$. In simpler terms, the model posits that the state $$ x(t) $$ changes according to the following linear relationship:

$$ x'(t) = A \cdot x(t) + B \cdot u(t) $$

Here, $$ x'(t) $$ represents the derivative of the state, or how the state changes over a small time increment $$ dt $$. The matrix $$ A $$ (of dimensions N×N) operates over the state $$ x(t) $$, while the matrix $$ B $$ (of dimensions N×1) operates over the input $$ u(t) $$.

As for the output $$ y(t) $$, it is derived from a linear combination of the current state $$ x(t) $$ and the input $$ u(t) $$, as follows:

$$ y(t) = C \cdot x(t) + D \cdot u(t) $$

In this equation, $$ C $$ is a 1×N matrix, and $$ D $$ is a scalar (1×1 matrix).

For simplification purposes, we can modify the model by omitting $$ D $$, resulting in the output $$ y(t) $$ being solely a function of the state $$ x(t) $$:

$$ y(t) = C \cdot x(t) $$

This simplification doesn't significantly limit the model's expressiveness, as the state $$ x(t) $$ can effectively incorporate information from the input $$ u(t) $$. This foundational model can later be expanded or stacked to represent more complex systems, as we'll explore in subsequent sections.

Mechanical Spring System: A State Space Model Example
-----------------------------------------------------

To further our understanding of state space models (SSM) and how they can be applied, let's step away from machine learning momentarily and delve into a classic mechanics scenario.

Consider the scenario of a mass attached to a spring, which in turn is attached to a wall. The position of this mass over time, denoted as $$ y(t) $$, changes as various forces (represented by $$ u(t) $$ ) are applied to it. This system is governed by three key parameters: the mass of the object $$ m $$, the spring constant $$ k $$, and the friction constant $$ b $$. These parameters interact to determine the movement of the mass on the spring.

The behavior of this system can be described by a differential equation that relates the force applied to the mass, the resistance due to friction, and the restoring force exerted by the spring:

$$ m y''(t) = u(t) - b y'(t) - k y(t) $$

Here, $$ y''(t) $$ represents the acceleration of the mass, $$ y'(t) $$ its velocity, and $$ y(t) $$ its position at time $$ t $$.

To represent this scenario as a state space model, we can rewrite the equation in matrix form. This involves defining matrices $$ A $$, $$ B $$, and $$ C $$ that encapsulate the relationships between these variables:

$$ A = \begin{bmatrix} 0 & 1 \\ -\frac{k}{m} & -\frac{b}{m} \end{bmatrix}, \quad B = \begin{bmatrix} 0 \\ \frac{1}{m} \end{bmatrix}, \quad C = \begin{bmatrix} 1 & 0 \end{bmatrix} $$

In this representation:
- Matrix $$ A $$ describes how the state of the system (comprising the position and velocity of the mass) evolves over time.
- Matrix $$ B $$ represents how the external force $$ u(t) $$ influences the state of the system.
- Matrix $$ C $$ relates the state of the system to the output, which in this case is the position $$ y(t) $$ of the mass.

This SSM effectively captures the dynamics of a mass on a spring and can be used to predict the system's behavior in response to various forces, illustrating the versatility of state space models in modeling real-world physical systems.

Discrete-Time State Space Models: A Recurrent Representation
------------------------------------------------------------

While state space models (SSMs) are traditionally formulated for continuous functions, real-world applications often involve discrete data points sampled at regular intervals. This is particularly true in fields like speech processing, where data is sampled at precise time steps (e.g. 40K samples per second). To adapt SSMs for such scenarios, we need to convert the continuous model into a discrete-time version.

Consider a discrete input sequence $$ (u_0, u_1, \ldots) $$, which can be thought of as samples from an underlying continuous signal $$ u(t) $$. Here, each discrete input $$ u_k $$ corresponds to the value of the continuous function $$ u(t) $$ at time $$ k\Delta $$, where $$ \Delta $$ is the step size representing the resolution of the input.

To discretize the continuous-time SSM, we use a technique such as the bilinear method. This approach transforms the continuous state matrix $$ A $$ into a discrete approximation $$ \bar{A} $$. The discrete SSM then takes the following form:

$$ \bar{A} = (I - \frac{\Delta}{2} \cdot A)^{-1} (I + \frac{\Delta}{2} \cdot A) $$
$$ \bar{B} = \left((I - \frac{\Delta}{2} \cdot A)^{-1} \cdot \Delta\right) B $$
$$ \bar{C} = C $$

Here, $$ I $$ is the identity matrix, and the matrices $$ \bar{A}, \bar{B}, \bar{C} $$ are the discrete equivalents of $$ A, B, C $$ respectively.


This transformation changes the SSM from a function-to-function mapping to a sequence-to-sequence mapping, where $$ u_k \mapsto y_k $$. In this discrete setting, the state equation becomes a recurrence in $$ x_k $$, allowing the discrete SSM to be computed similarly to a Recurrent Neural Network (RNN). In this framework, $$ x_k \in \mathbb{R}^N $$ can be seen as a hidden state with the transition matrix $$ \bar{A} $$:

$$ x_k = \bar{A} x_{k-1} + \bar{B} u_k $$
$$ y_k = \bar{C} x_k $$

This "step" function in the discrete SSM bears a resemblance to the step function of an RNN. Such a model can efficiently handle data like speech, where inputs are inherently discrete and sequential. By adapting the continuous model to a discrete context, we enable the application of state space models in a wider range of practical scenarios.



Discrete-Time State Space Models and Their Relation to RNNs
-----------------------------------------------------------

**Discretization of Continuous-Time SSM:**
To adapt the continuous-time state space model (SSM) for discrete inputs, such as in cases where data is sampled at 40,000 samples per second, we employ a technique like the bilinear method. This approach modifies the continuous state matrix $$ A $$ into a discrete approximation $$ \bar{A} $$. The resulting discrete SSM is formulated as follows:

$$ \bar{A} = (I - \frac{\Delta}{2} \cdot A)^{-1} (I + \frac{\Delta}{2} \cdot A) $$
$$ \bar{B} = \left((I - \frac{\Delta}{2} \cdot A)^{-1} \cdot \Delta\right) B $$
$$ \bar{C} = C $$

In these expressions, $$ I $$ is the identity matrix. The matrices $$ \bar{A}, \bar{B}, \bar{C} $$ represent the discrete equivalents of $$ A, B, C $$, respectively. Here, $$ \Delta $$ is the time step corresponding to the sampling rate (the inverse of 40,000 samples per second).

**RNN View of Discrete SSM:**
The discrete version of SSMs can be conceptualized as a form of linear Recurrent Neural Network (RNN). In this framework, the current state $$ x $$ at time step $$ t $$ is determined by the previous state at time step $$ t-1 $$ and the current input $$ u_t $$, while the output is derived from the current state. This relationship is expressed as:

- State Update: $$ x_t = \bar{A} x_{t-1} + \bar{B} u_t $$
- Output Calculation: $$ y_t = \bar{C} x_t $$

This linear RNN model shares several advantages with more complex RNNs like LSTMs and GRUs, notably:
- The state size at each time step is fixed ($$ N $$), allowing for consistent computation of output in terms of time and memory, irrespective of the context length.
- Unlike models such as transformers or CNNs, there is no "theoretical" limit on the context length in RNNs, meaning the entire history of data points can contribute to the current state calculation.

However, there are some drawbacks:
- RNNs, including this linear form, are susceptible to issues like the vanishing gradient problem, which can hinder effective training, especially for long sequences.
- RNNs lack training parallelism. In contrast to architectures like CNNs, where multiple layers or segments of data can be processed simultaneously, RNNs require sequential processing. This sequential dependency results in slower training as each step depends on the completion of the previous one.

**Addressing the Challenges:**
The next sections of our discussion will explore how the CNN perspective can aid in overcoming the challenges of training parallelism. Additionally, we will examine the HIPPO formulation as a method to address the issues related to gradient learning in RNNs. These advancements showcase how integrating insights from different model architectures can enhance the performance and applicability of sequence modeling techniques.

Training SSMs: Adopting a Convolutional Representation
------------------------------------------------------

**Transition from RNN to CNN through Unrolling:**
The recurrent formulation of state space models (SSMs), while conceptually straightforward, poses practical challenges for training on modern hardware, primarily due to its inherent sequential nature. However, there is a well-known connection between linear time-invariant (LTI) SSMs and continuous convolutions, allowing the recurrent SSM to be expressed as a discrete convolution.

**Unrolling the Recurrent Representation:**
Assuming an initial state of $$ x_{-1} = 0 $$, unrolling the recurrent equations of the SSM explicitly illustrates how each state and output can be computed in terms of the previous states and inputs. This process reveals a pattern akin to convolution operations:

- For $$ x_0 $$, the state is determined solely by $$ \bar{B} u_0 $$.
- For $$ x_1 $$, the state is influenced by both $$ \bar{A} \bar{B} u_0 $$ and $$ \bar{B} u_1 $$, and similarly for subsequent states.
- The output at each step, $$ y_k $$, is a function of the current state $$ x_k $$, following the linear relationship $$ y_k = \bar{C} x_k $$.

This sequential dependency can be vectorized into a convolution operation with an explicit formula for the convolution kernel. The kernel $$ \bar{K} $$ consists of terms like $$ \bar{C} \bar{A}^j \bar{B} $$ for $$ j = 0, 1, \ldots, L-1 $$, where $$ L $$ is the length of the kernel. This convolutional representation effectively turns the SSM into a form that can be processed as a CNN.

**SSM as a Convolutional Neural Network (CNN):**
The unrolled formula, which computes the output at time step $$ t $$ as a function of inputs from time steps $$ 0 $$ to $$ t $$, resembles the working mechanism of 1-D convolution kernels. In this perspective, the entire sequence of time steps can be processed through a CNN-like framework. This approach allows for training parallelism, provided the convolution kernel $$ \bar{K} $$ is precomputed.

In the CNN formulation, the computation of states and outputs at each time step can be visualized as the application of a 1-D convolution across the entire sequence. The convolution kernel $$ \bar{K} $$, which encapsulates the dynamics of the SSM, slides over the input sequence, computing the output at each step based on the weighted sum of the current and previous inputs.

**Advantages of the CNN Representation:**
- **Training Parallelism:** Unlike the sequential processing in RNNs, CNNs allow for parallel processing of data. This parallelism significantly speeds up training, especially for long sequences.
- **Efficient Kernel Calculation:** The upcoming section on the HIPPO formulation will discuss methods for efficiently calculating the convolution kernel $$ \bar{K} $$, further enhancing the practicality of using CNNs for training SSMs.

By reinterpreting the SSM as a convolutional operation, we can leverage the computational advantages of CNNs, such as parallel processing and efficient handling of long sequences. This transformation underscores the flexibility and adaptability of SSMs in various computational contexts, enabling their efficient implementation and training on contemporary hardware platforms.


HIPPO Matrices in State Space Models
------------------------------------

**The Concept of HIPPO Matrices:**
HIPPO (History Polynomial Projection Operator) matrices represent a novel approach in state space models (SSMs) to efficiently compress historical data points into a finite state. The core idea is to model the past data points using a finite state, in such a way that these points can be reconstructed from this state. One common method for achieving this is through polynomial approximation, where past states are represented using the coefficients of a polynomial.

**Polynomial Coefficients as State Representation:**
In the context of SSMs, the state $$ x_t $$ at any given time step $$ t $$ is represented by the coefficients of a polynomial. These coefficients are structured so that they can rebuild past data points. The transition matrix $$ A $$ in the model is designed to update the polynomial coefficients at $$ x_t $$ based on the previous coefficients at $$ x_{t-1} $$ and the new data point $$ u_t $$. This formulation of the HIPPO matrix emphasizes recent data points more, allowing the model to effectively utilize recent context in its predictions.

**Advantages of HIPPO Matrices:**
1. **Efficient State Updates:** The HIPPO matrix allows for updating the state to include new input data points at each time step through a single matrix multiplication. This operation is constant in time, irrespective of the length of the context.
2. **Calculation of Power Matrices:** A beneficial property of the HIPPO matrix is its ability to facilitate the computation of $$ A^N $$ matrices. This is particularly useful in deriving the convolutional kernel for the CNN representation of the SSM.
3. **Addressing Gradient Vanishing Problem:** Initializing the matrix $$ A $$ as per the HIPPO matrix design helps mitigate the vanishing gradient problem. The $$ A^N $$ matrices are well-defined and do not lead to exploding gradients, which is a common issue in traditional RNNs. This stability makes the HIPPO-based SSMs more robust and effective in learning from long sequences.

**Structured State Space for Sequence Modeling:**
The HIPPO matrix introduces a structured approach to sequence modeling, allowing for more efficient and robust handling of sequential data. This structured matrix in the state space model is key to addressing some of the common challenges faced in sequence modeling, such as effectively capturing long-term dependencies and ensuring stability in learning.

**Further Exploration:**
For a more detailed explanation and insights into the functioning and applications of HIPPO matrices in state space models, interested readers can refer to resources such as YouTube lectures and research papers that delve deeper into this topic. These resources provide comprehensive overviews and practical examples of how HIPPO matrices revolutionize the way we approach sequence modeling, offering a blend of theoretical understanding and real-world applicability.

Recap: Speech Waveform Modeling with State Space Models
-------------------------------------------------------

**Understanding Speech Waveforms:**
Speech waveform modeling focuses on capturing the variation of air pressure over time, represented by the function $$ y(t) $$, which denotes the amplitude of the sound wave at any given time $$ t $$. This function is essentially continuous and one-dimensional. In practical applications, like digital recording, this continuous waveform is converted into a discrete sequence. For instance, sampling at 40,000 samples per second translates the waveform into a discrete 1-D sequence, where each sample corresponds to a specific time step $$ dt $$.

**Generative Modeling in Speech Waveforms:**
In state space models, we usually discuss an input function $$ u $$ and an output function $$ y $$. However, in speech waveform modeling, we primarily deal with a single function, $$ y(t) $$. The objective here is to predict the amplitude of the sound waveform at time $$ t $$ using the values of $$ y $$ from previous time samples (0, ..., $$ t-1 $$). This translates to setting the input function $$ u(t) $$ as $$ y(t-1) $$, with $$ u(0) = 0 $$, and attempting to predict $$ y(t) $$ from $$ u(t) $$ to $$ u(0) $$, or equivalently, from $$ y(t-1) $$ to $$ y(0) $$. This approach is akin to autoregressive or generative modeling in deep learning.

**Challenges with Different Models:**
- **Transformers:** When modeling raw speech waveform data, especially over long contexts (e.g., 400K time steps for a 10-second sample), transformers face computational challenges. The attention mechanism scales quadratically with context length ($$ O(L^2) $$), making training for long contexts prohibitively expensive in terms of computation and memory.
- **RNNs:** While RNNs can model sequential data, their training time scales linearly with context length. Additionally, they lack easy parallelization as the states need to be computed sequentially for each time step.
- **CNNs:** CNNs are limited by finite context lengths and thus struggle to model very long sequences effectively.

**Advantages of S4 Models:**
S4 (State Space Sequence Models) combine the efficient training properties of CNNs with the sequential modeling strength of RNNs:
- **Efficient Training:** Using HIPPO matrices, the convolutional kernel for S4 models can be precomputed, enabling parallel training without the need to calculate states for each time step individually.
- **Inference Efficiency:** For inference, the RNN-like structure of S4 models allows for state and output calculation at each time step without the need for time-intensive convolutional or transformer-based operations.
- **Flexibility and Complexity:** Although S4 models are fundamentally linear RNNs, incorporating them as layers in more complex deep learning architectures with nonlinear interlayers allows for modeling of more intricate state dynamics and sequences.

**Extending to N-Dimensional Models:**
The discussion so far has centered on 1-D models suitable for speech waveform modeling. However, for applications involving multi-layered or multi-dimensional inputs like text and images, S4 models can be extended to N dimensions. A straightforward approach is to model each dimension with a separate S4 state and concatenate the outputs to form an N-dimensional output. Subsequent fully connected layers, possibly with nonlinear components, can model inter-dimensional dependencies. This approach resembles depth-wise separable convolutions in traditional CNN architectures.

This multi-faceted approach to speech waveform modeling using state space models, particularly S4 models, illustrates the adaptability and efficacy of these models in handling complex, high-dimensional sequential data.
