---
layout: post
title:  "Retrieval Augmented Generation"
date:   2024-02-03 11:53:18 +0530
categories: AI
---

## Table of Contents
1. [Retrieval Augmented Generation](#retrieval-augmented-generation)
1. [Methodology](#methodology)
1. [Embedding-Based Retrieval](#embedding-based-retrieval)
1. [Different Levels of RAG Considerations](#different-levels-of-rag-considerations)
1. [End-to-End Training](#end-to-end-training)
1. [Updating embeddings while training](#updating-embeddings-while-training)
1. [Conclusion](#conclusion)
1. [References](#references)

Retrieval Augmented Generation
--------------------------------------------------------



Large Language Models (LLMs) have shown remarkable proficiency in generating text and performing various Natural Language Processing (NLP) tasks. Despite their capabilities, LLMs face notable challenges. Primarily, all information is encoded within the model's parameters, making the integration of new information challenging. This limitation can lead to increased occurrences of inaccuracies or 'hallucinations.'

One strategy to mitigate these issues is through in-context learning. Leveraging in-context information facilitates the incorporation of new data and reduces the likelihood of hallucinations, thereby enhancing the model's ability to answer questions more accurately.

To further augment the LLM's capabilities, we can introduce information from external knowledge bases or documents. This approach essentially extends the LLM in a non-parametric manner, meaning the model isn't solely reliant on its internal weights (parametric) to generate responses. Instead, it can access a vast array of external information (non-parametric), overcoming the intrinsic limitations posed by the model's fixed context size. However, a critical challenge remains: how to efficiently retrieve relevant information to enrich the context based on the query at hand.

This challenge gives rise to the concept of Retrieval-Augmented Generation (RAG). RAG addresses the need for a sophisticated mechanism to select and integrate pertinent external information into the LLM's context, thereby significantly enhancing its response accuracy and reliability.


Methodology
--------------------------------------------------------

In the context of enhancing Large Language Models (LLMs) with external information, let's consider a scenario where we have $$N$$ documents available alongside our LLM. This setup leverages two key components:

1. **Retriever** $$(p_\eta(z\\|x))$$: This component, parameterized by $$\eta$$, is designed to return top-$$K$$ truncated distributions over text passages for a given query $$x$$. It efficiently selects relevant documents or passages that are likely to contain information pertinent to the query.

2. **Generator** $$(p_\theta(y_i\\|x, z, y_{1:i-1}))$$: Parameterized by $$\theta$$, the generator produces the current token $$y_i$$ based on the context of the previous $$i - 1$$ tokens $$(y_{1:i-1})$$, the original input $$x$$, and a retrieved passage $$z$$.

When it comes to integrating the retrieved documents $$Z$$ to respond to the input $$x$$, there are principally two methodologies:

1. **Sequence-Level Combination**: Here, the generator computes the output sequence probability for each retrieved document. These probabilities are then aggregated to determine the overall sequence probability. This method can be mathematically represented as:

$$p_{\text{RAG-Sequence}}(y\\|x) \approx \sum_{z \in \text{top-k}(p(\cdot\\|x))} p_\eta(z\\|x) \prod_{i=1}^{N} p_\theta(y_i|x, z, y_{1:i-1})$$

In this approach, the focus is on evaluating the best sequence as a whole, considering each document's contribution to the final generated sequence.

2. **Token-Level Combination**: This method defines the probability of generating a sequence by considering the contribution of each retrieved document at every token generation step. It is formalized as:

$$p_{\text{RAG-Token}}(y|x) \approx \prod_{i=1}^{N} \sum_{z \in \text{top-k}(p(\cdot|x))} p_\eta(z|x) p_\theta(y_i|x, z, y_{1:i-1})$$

Here, the emphasis is on the generation of individual tokens, with each token's probability being influenced by the aggregation of probabilities across all relevant documents.


Embedding-Based Retrieval
--------------------------------------------------------

In Retrieval-Augmented Generation (RAG) systems, the capability to augment Large Language Models (LLMs) with pertinent external information is crucially dependent on advanced document retrieval methods. The retriever component, $$p_\eta(z\\|x)$$, is tasked with identifying the top-$$K$$ documents most relevant to the input query $$X$$ by utilizing a sophisticated embedding-based approach. This method is underpinned by the principles of Dense Passage Retrieval (DPR) and Maximum Inner Product Search (MIPS), described as follows:

- **ENCODING** utilizes a encoder architecture to generate dense embeddings for both documents and queries. This approach allows for the efficient and effective retrieval of information by directly comparing the embeddings of the query with those of potential documents. The DPR model can be formalized as:

$$p_\eta(z\\|x) \propto \exp(\text{encode}(d(z))^\top \text{encode}(q(x)))$$

where $$\text{encode}(d(z))$$ provides a dense representation of a document, and $$\text{encode}(q(x))$$ provides a corresponding dense representation of the query. These embeddings are produced by encoders based on Large Language Models, which capture the nuanced semantics of the text.

- **RETREIVAL** is then employed to efficiently find the documents whose embeddings are most similar to the query embedding. By optimizing for the inner product between the query and document embeddings, MIPS facilitates the selection of documents that are most likely to contain relevant information for answering the query.

This integrated approach leverages the strengths of LLM-based embeddings for understanding and matching the content, with the efficiency of MIPS for the retrieval process. It ensures that the retrieved documents are highly relevant, thereby enhancing the quality and relevance of the generated responses by the LLM.



Sampling in RAG Models
--------------------------------------------------------

- **RAG-Token**: Utilizes an autoregressive seq2seq generation approach, where each token's generation probability integrates information from top-$$k$$ retrieved documents. Decoding is achieved through standard beam search, leveraging the aggregated transition probabilities from the retrieved documents to guide the generation of each token.

- **RAG-Sequence**: Does not employ a per-token likelihood approach due to its focus on sequence-level coherence with retrieved documents. Instead, it performs a separate beam search for each retrieved document, scoring entire sequences to select the most contextually appropriate response based on the document-specific information.


Expanding on the considerations for implementing Retrieval-Augmented Generation (RAG) models, we can delve into the nuances of fine-tuning, efficient retrieval strategies, and embedding updates. Each of these factors plays a crucial role in optimizing RAG models for specific tasks or datasets.

Different Levels of RAG Considerations
--------------------------------------------------------

1. **Fine-tuning the LLM for RAG + Data**: Tailor the LLM to RAG by training it on a mix of input queries and outputs, enriched with domain-specific data. This fine-tuning enhances the model's ability to leverage retrieved context effectively.

2. **Fine-tuning the Retriever**: Boost the retriever's precision by training it on datasets with marked pairs of queries and relevant documents. Modifications to its architecture or embedding mechanism can further refine its document selection capabilities.

3. **Efficient Retrieval Without Constant Embedding Recalculation**: Reduce computational demands by using cached pre-computed document embeddings and updating only when new or changed content is introduced. This approach avoids the need for constant re-calculation.

4. **Updating Embeddings**: Maintain up-to-date embeddings by periodically refreshing them for newly added or significantly modified documents, employing strategies such as continual learning for minimal disruption.

### Limitations with Fixed Models and APIs
In scenarios where customization of the LLM or retriever is restricted, such as with API-based models like OpenAI's GPT, adaptability is constrained. Strategies to mitigate this include merging relevant information into the input prompt to enhance the model's response quality and employing creative prompt adjustments to indirectly influence the output. Despite these limitations, GPT's advanced capabilities can often offset the lack of deep retrieval process customization, still delivering coherent and contextually appropriate responses.


End-to-End Training
--------------------------------------------------------

The "End-to-End Training of Neural Retrievers for Open-Domain Question Answering" introduces a comprehensive approach to optimizing RAG systems, focusing on a unified training methodology that simultaneously enhances both the retriever and generator components. Leveraging domain-specific datasets, the model is trained to accurately identify and utilize pertinent documents, improving the system's response relevance and accuracy. The deployment of specialized encoder and decoder models for questions and documents allows for nuanced optimization through targeted pretraining and fine-tuning. The model's discriminating capabilities are further honed by incorporating hard negative training with BM25, teaching it to distinguish between non-answer containing documents that are superficially relevant. This advanced strategy marks a substantial improvement in the field of open-domain question answering, aiming to significantly elevate response quality. For more details, the research can be found at [https://arxiv.org/pdf/2101.00408v2.pdf](https://arxiv.org/pdf/2101.00408v2.pdf), providing a deep dive into the methodology and its implications.


Updating embeddings while training
--------------------------------------------------------

Fine-tuning the entire Retrieval-Augmented Generation (RAG) architecture, including the DPR retriever for question-answering, requires efficiently updating document embeddings without slowing down the training loop. Utilizing HuggingFace Datasets, Pytorch-Lightning, and Python Multiprocessing libraries enables parallel processing of re-encoding and re-indexing tasks alongside main training activities. This approach ensures that the embedding updates do not become a bottleneck, maintaining high efficiency and speed in model training. By executing re-encoding and re-indexing in parallel, the system can swiftly incorporate new information and adapt its retrieval capabilities, significantly enhancing question-answering performance. A detailed implementation guide, including practical insights and code examples, is available in a Medium post, providing a valuable resource for practitioners looking to optimize RAG models for question-answering tasks.


Conclusion
--------------------------------------------------------

The survey on Retrieval-Augmented Generation (RAG) offers an insightful overview of RAG's fundamental principles and its evolution, highlighting the synergy between traditional language modeling and external information retrieval to improve tasks like question-answering. It emphasizes the initial research that introduced RAG concepts and the subsequent development of various libraries and tools that support RAG implementation. This resource is instrumental for those looking to delve into RAG models, providing a solid foundation on its operations and the technological ecosystem surrounding it. For a comprehensive understanding, the survey is available at [https://arxiv.org/pdf/2312.10997v4.pdf](https://arxiv.org/pdf/2312.10997v4.pdf), detailing the advancements and applications of RAG in natural language processing.




### References

- Lewis, P., et al. (2020). **Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks**. Available at [arxiv.org](https://arxiv.org/pdf/2005.11401v4.pdf).

- Karpukhin, V., et al. (2021). **End-to-End Training of Neural Retrievers for Open-Domain Question Answering**. Available at [arxiv.org](https://arxiv.org/pdf/2101.00408v2.pdf).

- Tay, Y., et al. (2023). **Benchmarking Large Language Models in Retrieval-Augmented Generation**. Available at [arxiv.org](https://arxiv.org/pdf/2309.01431v2.pdf).

- Izacard, G., et al. (2023). **Retrieval-Augmented Generation for Large Language Models: A Survey**. Available at [arxiv.org](https://arxiv.org/pdf/2312.10997v4.pdf).

- Weng, L. (2023). **Augmented Language Models**. Retrieved from [lilianweng.github.io](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#augmented-language-models).

