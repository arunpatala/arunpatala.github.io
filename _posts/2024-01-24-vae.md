---
layout: post
title:  "Variational Auto Encoders"
date:   2024-01-23 11:53:18 +0530
categories: AI
---

## Table of Contents
1. [Auto Encoders](#auto-encoders)
2. [Variational Auto Encoders](#variational-auto-encoders)
3. [Loss Function : The ELBO Equation](#loss-function-the-elbo-equation)
4. [Example](#example)
4. [Advantages](#advantages)
4. [References](#references)
4. [Appendix](#appendix)

Auto Encoders
--------------------------------------------------------

Auto encoders are a fascinating tool in data compression, particularly useful for distilling complex data into a more manageable, low-dimensional format. This technique is crucial for highlighting significant characteristics within a dataset. Imagine we have a collection of facial images, each sized at 256x256 pixels. Auto encoders can transform these images into a compact vector of just 32 features, effectively capturing critical aspects like facial expressions or head poses within these condensed variables.

The process involves two main components: an encoder and a decoder. The encoder is a neural network that receives an input image (x) and converts it into a latent, or hidden, representation (z). Mathematically, this is represented as:

$$ z = \text{f}_{\text{enc}}(x) $$

However, the challenge lies in determining the encoder's parameters using a given dataset (X1:N). To address this, we introduce a decoder, which essentially reverses the encoder's function. It takes the latent variable (z) and reconstructs an image (x'), aiming to match the original input (x) as closely as possible:

$$ x' = \text{f}_{\text{dec}}(z) \text{ where } z = \text{f}_{\text{enc}}(x) $$

The quality of reconstruction is measured using a loss function, typically the Mean Squared Error (MSE), which calculates the difference between the reconstructed image (x') and the original image (x):

$$ \text{loss} = \text{MSE}(x' - x) $$

Optimizing this loss function helps in learning latent representations that accurately encode the input image. It's important to ensure that the latent representation is significantly smaller than the input data size. If not, the encoding process might simply memorize the input data rather than learning to distill its essential features.

Beyond basic auto encoders, there are more sophisticated variations like denoising auto encoders. These introduce noise to the input data (x), enabling the model to learn more robust and resilient latent representations, particularly effective in handling outliers. Additional enhancements can include incorporating loss parameters to ensure that the learned latent features are independent and disentangled, or promoting sparsity in the latent representation to further aid in feature disentanglement.

In summary, auto encoders provide an efficient means of capturing the essence of complex datasets, making them an invaluable tool in the world of data compression and feature extraction.

#### Understanding a Simple Autoencoder



```python
class Autoencoder(nn.Module):
    def __init__(self):
        super(Autoencoder, self).__init__()
        # Encoder part
        self.encoder = nn.Sequential(
            nn.Linear(28 * 28, 256),
            nn.ReLU(True),
            nn.Linear(256, 32),
            nn.ReLU(True))
        # Decoder part
        self.decoder = nn.Sequential(
            nn.Linear(32, 256),
            nn.ReLU(True),
            nn.Linear(256, 28 * 28),
            nn.Sigmoid())

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

criterion = nn.MSELoss()

```


An autoencoder is a neural network architecture comprised of two main components: the encoder and the decoder. The encoder part of the network compresses input data (in this case, an image of size 28x28 pixels) into a smaller, more dense representation, reducing it down to a 32-dimensional space using a two-layer neural network. This process is facilitated by the ReLU (Rectified Linear Unit) activation function, which introduces non-linearity, enabling the network to capture complex patterns in the data. The decoder, mirroring the structure of the encoder, works to reconstruct the input data from this compressed form, aiming to produce an output as close to the original input as possible. The reconstruction quality is measured using the Mean Squared Error Loss (MSELoss), which quantifies the difference between the original and reconstructed images. The networkâ€™s objective during training is to minimize this loss, thereby enhancing the accuracy of the reconstruction. The final layer of the decoder employs a Sigmoid activation function to ensure that the output values are appropriately scaled between 0 and 1, which is particularly useful for image data.

Variational Auto Encoders
--------------------------------------------------------

Variational Auto Encoders (VAEs) represent a significant evolution from traditional auto encoders, primarily due to their integration of probability distributions in the encoding process. Unlike standard auto encoders, which map an input data sample $$ x $$ to a single latent variable $$ z $$, VAEs convert $$ x $$ into a probability distribution $$ P(Z\\|X) $$. Typically, this distribution is Gaussian, characterized by a mean $$ \mu $$ and variance $$ \sigma^2 $$, with each latent variable within the Gaussian distribution considered to be independent:

$$ Q(Z) = \mathcal{N}(\mu, \sigma) $$

In this notation, $$ \mathcal{N} $$ indicates the normal (Gaussian) distribution. The encoder in a VAE ($$ \text{f}_{\text{enc}} $$) is designed to output these parameters: the mean $$ \mu $$ (of size N) and sigma $$ \sigma $$ (also of size N), which are derived from the input $$ X $$:

$$ \mu, \sigma = \text{F}_{\text{encoder}}(X) $$

From the Gaussian distribution $$ Q(Z\\|X) $$, the latent variable $$ Z $$ can be sampled. The decoding process in VAEs mirrors that in traditional auto encoders, aiming to reconstruct the input to produce:

$$ X' = \text{F}_{\text{decoder}}(Z) $$

Although the decoder's output could also be conceptualized as a Gaussian distribution, for simplicity, it is often treated as a single image. The reconstruction loss is measured using the Mean Squared Error (MSE) between the original input $$ X $$ and the reconstructed output $$ X' $$:

$$ \text{reconstruction loss} = \text{MSE}(X, X') $$

A key feature of VAEs is the ability to sample directly from the latent representation without an encoder. This is enabled by assuming that $$ Z $$ follows a standard Gaussian distribution with a mean of 0 and a deviation of 1. Therefore, the encodings $$ Z $$ (z1, ... zn) of the training dataset (x1, ... xn) conform to this standard normal distribution, which is recognized as the prior distribution of $$ Z $$, denoted as $$ P(Z) $$.

Through this advanced approach, VAEs offer a more nuanced and probabilistic method for data encoding and decoding, facilitating the generation of new data points and deeper insights into data structures.

#### Sample Code

```python
class VariationalAutoencoder(nn.Module):
    def __init__(self):
        super(VariationalAutoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(28 * 28, 400),
            nn.ReLU(True),
            nn.Linear(400, 40))
        self.decoder = nn.Sequential(
            nn.Linear(20, 400),
            nn.ReLU(True),
            nn.Linear(400, 28 * 28),
            nn.Sigmoid())

    def reparametrize(self, mu, logvar):
        var = logvar.exp()
        std = var.sqrt()
        eps = torch.cuda.FloatTensor(std.size()).normal_()
        return eps.mul(std).add(mu)

    def forward(self, x):
        h = self.encoder(x)
        mu = h[:, :20]
        logvar = h[:, 20:]
        z = self.reparametrize(mu, logvar)
        x_hat = self.decoder(z)
        return x_hat, mu, logvar

```
The `VariationalAutoencoder` class in the provided code is a specialized type of autoencoder that employs a probabilistic approach to encode inputs into a distribution rather than a fixed point. The encoder network, consisting of linear layers and ReLU activations, maps the input image (28x28 pixels) to a 40-dimensional space, which is then split into two parts representing the mean (`mu`) and log-variance (`logvar`). The key feature of this architecture is the `reparametrize` function, which uses these parameters to sample from the latent distribution, enabling the network to perform the reparameterization trick essential for backpropagation in stochastic models. The decoder then reconstructs the input from this sampled latent variable. Typically used in conjunction with a loss function that combines reconstruction error and a regularization term (Kullback-Leibler divergence), this model structure allows for learning a distribution over the latent space, providing a richer and more flexible representation of the input data.


Loss Function : The ELBO Equation
--------------------------------------------------------

In Variational Auto Encoders (VAEs), a crucial component in achieving effective learning is the incorporation of a special type of loss known as the Kullback-Leibler (KL) divergence loss. This loss measures the difference between two probability distributions: the distribution $$ Q(Z\\|X) $$ produced by the encoder, and the prior distribution $$ P(Z) $$, typically a standard Gaussian distribution.

The total loss function in VAEs is represented by the Evidence Lower Bound (ELBO). The ELBO is formulated as follows:

$$ \log P(X) \geq \mathbb{E}_{Q(Z\\|X)}[\log P(X\\|Z)] - D_{KL}(Q(Z\\|X) \| P(Z)) $$

This equation encompasses two primary components:

1. **Reconstruction Loss**: The first term, $$ \mathbb{E}_{Q(Z\\|X)}[\log P(X\\|Z)] $$, is the reconstruction loss. It represents the expectation of the log likelihood of reconstructing $$ X $$ from $$ Z $$, which is the latent representation obtained from $$ X $$. In practice, this term encourages the VAE to accurately reconstruct the input data, and it can be approximated by the sum of Mean Squared Errors (MSE) over the training dataset, i.e., the sum of MSE(X, X') for all data points.

2. **KL Divergence**: The second term, $$ D_{KL}(Q(Z\\|X) \| P(Z)) $$, is the KL divergence between the learned distribution $$ Q(Z\\|X) $$ and the prior distribution $$ P(Z) $$. This term acts as a regularizer, ensuring that the learned distribution $$ Q(Z\\|X) $$ does not deviate significantly from the prior distribution $$ P(Z) $$, which is often a standard Gaussian distribution. The KL divergence pushes the encoder to learn representations $$ Z $$ that are as close as possible to the standard Gaussian distribution, helping in the generalization of the model.

The ELBO essentially represents a trade-off between these two components: the accuracy of the reconstruction and the regularity of the learned latent space. By maximizing the ELBO, a VAE learns to reconstruct the input data accurately while ensuring that the latent variables $$ Z $$ maintain a distribution close to the Gaussian prior. This balance is key to the effectiveness of VAEs in generating new data points that are both diverse and representative of the original dataset.


Example
--------------------------------------------------------


Consider an example where we have a data sample $$ x_1 $$. The encoder function ($$ \text{f}_{\text{enc}} $$) processes this sample to yield a mean ($$ \mu_1 $$) and a sigma ($$ \sigma_1 $$):

$$ \mu_1, \sigma_1 = \text{f}_{\text{enc}}(x_1) $$

This pair $$ (\mu_1, \sigma_1) $$ defines a Gaussian distribution from which we can sample a latent variable $$ z_1 $$, represented as $$ z_1 = N(\mu_1, \sigma_1) $$. The decoder ($$ \text{f}_{\text{dec}} $$) then takes $$ z_1 $$ to reconstruct $$ x_1 $$, denoted as $$ x_1' $$:

$$ x_1' = \text{f}_{\text{dec}}(z_1) $$

The total loss for this data sample is the sum of the reconstruction loss (typically Mean Squared Error between $$ x_1' $$ and $$ x_1 $$) and the KL divergence loss, which measures how much the distribution $$ Q(Z\\|X) $$ (represented by $$ \mu_1 $$ and $$ \sigma_1 $$) diverges from the prior distribution $$ P(Z) $$:

$$ \text{loss} = \text{MSE}(x_1', x_1) + \text{KL loss} (Q|P) $$

#### Reparameterization Trick

To optimize the encoder and decoder parameters effectively, we need to compute gradients with respect to $$ \mu_1 $$ and $$ \sigma_1 $$. However, direct sampling from the distribution $$ N(\mu_1, \sigma_1) $$ does not allow gradients to flow back through $$ \mu_1 $$ and $$ \sigma_1 $$. This is where the reparameterization trick comes in.

Instead of directly sampling $$ z_1 $$, we sample an auxiliary variable $$ e $$ from a standard normal distribution $$ N(0,1) $$. Then, $$ z_1 $$ is computed as:

$$ z_1 = \mu_1 + e \times \sigma_1 $$

With this formulation, the gradients can flow back through $$ z_1 $$ to $$ \mu_1 $$ and $$ \sigma_1 $$, enabling the optimization of the encoder and decoder parameters using gradient-based methods. This trick is a cornerstone in training VAEs, as it allows for effective backpropagation while maintaining the stochastic nature of the latent variable sampling.

#### Sample Code

```python
        x_hat, mu, logvar = model(img)
        NKLD = mu.pow(2).add(logvar.exp()).mul(-1).add(logvar.add(1))
        KLD = torch.sum(NKLD).mul(-0.5)
        KLD /= 128 * 784
        loss = BCE(x_hat, img) + KLD

```

In the provided code, an image is processed through a Variational Autoencoder model to generate a reconstructed image and parameters of the latent distribution (mean and log-variance). The Kullback-Leibler Divergence (KLD) is calculated to measure the deviation of the latent distribution from a standard normal distribution. This involves operations on the mean and log-variance, and the resulting KLD is normalized by the batch size and image dimensions. The final loss is a sum of the Binary Cross Entropy (BCE) between the original and reconstructed images, and this normalized KLD, combining reconstruction accuracy and latent space regularization.

Advantages
--------------------------------------------------------
Variational Auto Encoders (VAEs) offer several key benefits in generative modeling:

1. **Simple Gaussian Sampling**: VAEs enable sampling of the latent vector \( z \) from a standard Gaussian distribution \( N(0,1) \), simplifying the generation of new data points.

2. **Generation of Novel Data**: Without needing specific inputs \( x \), VAEs can create unseen samples, diversifying the range of generated data beyond the training dataset.

3. **Manipulation of Latent Features**: VAEs provide the ability to control and adjust latent variables, such as altering facial expressions in image data from smiling to frowning.

4. **Extrapolation Capabilities**: They allow for extrapolation and manipulation of latent vectors, enabling the exploration and generation of data points beyond the variations in the training data.

In essence, VAEs are powerful for their flexible and controlled data generation, leveraging a probabilistic approach to create diverse and novel data outputs.


References
--------------------------------------------------------

1. [Auto Encoders](https://reyhaneaskari.github.io/AE.htm)
2. [Stanford Lecture \| Generative Models](https://www.youtube.com/watch?v=5WoItGTWV54)
3. [MIT 6.S191: Deep Generative Modeling](https://www.youtube.com/watch?v=3G5hWM6jqPk)
4. [From Autoencoder to Beta-VAE](https://lilianweng.github.io/posts/2018-08-12-vae/)

<a id="appendix"></a>
## Appendix

## Table of Contents
1. [Distributions X and Z](#distributions-x-and-z)
2. [Prior and Posterior Distributions](#prior-and-posterior-distributions)
3. [Variational Distributions](#variational-distributions)
4. [Kullback-Leibler (KL) Divergence](#kullback-leibler-kl-divergence)
5. [Kullback-Leibler (KL) Divergence for Two Gaussians](#kullback-leibler-kl-divergence-for-two-gaussians)
6. [Reconstruction Loss](#reconstruction-loss)
7. [ELBO (Evidence Lower Bound)](#elbo-evidence-lower-bound)


<a id="appendix1"></a>

Distributions X and Z
--------------------------------------------------------

let's clarify the notation and the meaning of the variables in the equation $$ P(X) = \int P(X, Z) dZ $$:

- **$$ X $$**: This represents the observed data samples. In the context of a VAE, $$ X $$ would be the input data that you're trying to model, such as images, text, etc.

- **$$ Z $$**: These are the latent variables. In a VAE, the latent variables are typically assumed to follow some prior distribution, often chosen to be a Gaussian distribution for mathematical convenience and tractability. However, $$ Z $$ itself in this context is not the Gaussian distribution; it's a set of variables that are assumed to be drawn from this distribution.

- **$$ P(X, Z) $$**: This is the joint probability distribution of the observed data $$ X $$ and the latent variables $$ Z $$. It represents how likely it is to observe the data $$ X $$ together with a particular configuration of the latent variables $$ Z $$.

- **The Integral $$ \int P(X, Z) dZ $$**: This integral is over the latent variable space and is used to marginalize out the latent variables. Essentially, it sums (or integrates) over all possible values of $$ Z $$, weighing by their probability, to give you the marginal likelihood of the observed data $$ X $$. This marginal likelihood is what we ultimately want to maximize in a generative model like a VAE, but it's often intractable to compute directly, hence the need for variational methods.

So in summary, $$ X $$ is your data, $$ Z $$ are latent variables that are assumed to follow a distribution (often Gaussian), and $$ P(X) = \int P(X, Z) dZ $$ is the marginal likelihood of your data, obtained by integrating out the latent variables from the joint distribution.



<a id="appendix2"></a>

Prior and posterior distributions
--------------------------------------------------------

The concepts of prior and posterior distributions are fundamental in Bayesian statistics and are crucial to understanding variational autoencoders (VAEs). Let's define these terms generally and then apply them to the context of VAEs.

#### General Definitions

1. **Prior Distribution**: 
   - The prior distribution represents our beliefs about an unknown parameter or variables before observing any data. It's essentially our assumption or model about how we think these parameters behave in the absence of any specific data.
   - Mathematically, a prior distribution over a variable $$ Z $$ is denoted as $$ P(Z) $$. This distribution is "prior" in the sense that it is specified before incorporating the observed data.

2. **Posterior Distribution**:
   - The posterior distribution represents our updated beliefs about the same parameter or variables after observing data. It's a combination of our prior belief and the new information provided by the data.
   - Mathematically, given observed data $$ X $$ and a latent variable $$ Z $$, the posterior distribution of $$ Z $$ given $$ X $$ is denoted as $$ P(Z\\|X) $$. This is calculated using Bayes' theorem, which combines the likelihood of the data given the parameter (or variable), $$ P(X\\|Z) $$, with the prior, $$ P(Z) $$, to produce the posterior, $$ P(Z\\|X) $$.

#### In the Context of VAEs

- **Prior Distribution**: 
  - In a VAE, the prior distribution is typically assumed for the latent variables $$ Z $$. A common choice is a multivariate Gaussian distribution with a mean of zero and a unit covariance matrix, denoted as $$ \mathcal{N}(0, I) $$. This assumption simplifies calculations and encourages a well-structured latent space.

- **Posterior Distribution**:
  - In the context of a VAE, the true posterior distribution $$ P(Z\\|X) $$ is the distribution of the latent variables given the observed data. It tells us about the distribution of the latent variables that could have generated the observed data.
  - However, this true posterior is often intractable to compute directly. Therefore, in VAEs, we approximate it with a variational distribution $$ Q(Z\\|X) $$, which is typically chosen to be a Gaussian whose parameters are learned from the data using the encoder part of the VAE.

In summary, in the context of VAEs, the prior distribution over the latent variables is often a simple Gaussian, reflecting a lack of specific knowledge about the structure of the latent space before observing any data. The posterior distribution, which is more complex and data-dependent, is approximated by another Gaussian whose parameters are learned to best explain the observed data while remaining as close as possible to the prior. This approximation allows for efficient training and inference in VAEs.


<a id="appendix3"></a>

Variational Distributions
--------------------------------------------------------

The concept of a variational distribution is central to variational inference, a method used in Bayesian statistics and machine learning for approximating complex probability distributions. Let's define it in general and then in the specific context of Variational Autoencoders (VAEs).

#### General Meaning of Variational Distribution

1. **Definition**: A variational distribution is a simpler, parameterized probability distribution used to approximate a more complex or intractable distribution. This approximation is central to variational inference.
   
2. **Purpose**: The goal is to choose a variational distribution that is computationally tractable and can closely approximate the target distribution.

3. **Optimization**: The parameters of the variational distribution are optimized to make the distribution as close as possible to the target distribution. This optimization often involves minimizing some form of divergence (like the Kullback-Leibler divergence) between the two distributions.

#### Variational Distribution in the Context of VAEs

In the context of VAEs, the variational distribution plays a crucial role:

1. **Target Distribution**: The target distribution in VAEs is the posterior distribution of the latent variables given the observed data, denoted as $$ P(Z\\|X) $$. This distribution is typically complex and intractable due to the non-linear and high-dimensional nature of the data and model.

2. **Variational Distribution**: 
   - In VAEs, the variational distribution, usually denoted as $$ Q(Z\\|X) $$, is used to approximate the intractable true posterior $$ P(Z\\|X) $$.
   - It is parameterized, often by a neural network (the encoder part of the VAE), which learns to map the input data $$ X $$ to the parameters of $$ Q(Z\\|X) $$. This distribution is typically chosen to be a Gaussian with a diagonal covariance matrix for computational simplicity.

3. **Optimization**: 
   - The parameters of the neural network are optimized to make $$ Q(Z\\|X) $$ as close as possible to the true posterior $$ P(Z\\|X) $$.
   - This is usually done by maximizing the Evidence Lower Bound (ELBO), which involves both the reconstruction quality of the data and the Kullback-Leibler (KL) divergence between $$ Q(Z\\|X) $$ and the prior distribution of $$ Z $$.

4. **Role in VAEs**: 
   - The variational distribution enables the efficient training and operation of VAEs. It allows for the sampling of latent variables in a way that is differentiable and suitable for backpropagation, thanks to techniques like the reparameterization trick.

In summary, in the context of VAEs, the variational distribution is a neural network-parameterized distribution that approximates the complex posterior distribution of the latent variables given the observed data. Its optimization via techniques like the ELBO and the reparameterization trick is what makes VAEs powerful tools for learning complex data distributions in an efficient and scalable way.


<a id="appendix4"></a>

Kullback-Leibler (KL) divergence
--------------------------------------------------------

The Kullback-Leibler (KL) divergence is a concept from information theory, widely used in statistics and machine learning, including in the context of Variational Autoencoders (VAEs).

#### KL Divergence in General

1. **Definition**: KL divergence is a measure of how one probability distribution diverges from a second, expected probability distribution. It's often used to measure the difference between two distributions.

2. **Mathematical Expression**: For two discrete probability distributions $$ P $$ and $$ Q $$, the KL divergence from $$ Q $$ to $$ P $$ is defined as:
   $$ D_{KL}(P || Q) = \sum_x P(x) \log\left(\frac{P(x)}{Q(x)}\right) $$
   For continuous distributions, the sum is replaced by an integral.

3. **Properties**:
   - **Non-Negativity**: $$ D_{KL}(P \\| Q) \geq 0 $$. It is zero if and only if $$ P $$ and $$ Q $$ are the same distribution (in the case of discrete variables) or almost everywhere equal (in the case of continuous variables).
   - **Asymmetry**: It is not symmetric, meaning $$ D_{KL}(P \\| Q) $$ is not necessarily equal to $$ D_{KL}(Q \\| P) $$.

4. **Interpretation**: It can be interpreted as a measure of the amount of information lost when $$ Q $$ is used to approximate $$ P $$. It's often described as the "distance" between two distributions, although it's not a true distance metric since it's not symmetric and doesn't satisfy the triangle inequality.

#### KL Divergence in the Context of VAEs

In VAEs, KL divergence plays a crucial role in the training process:

1. **Target Distributions**: The two distributions in the context of a VAE are:
   - $$ Q(Z\\|X) $$: The variational distribution (approximation) of the latent variables $$ Z $$ given the data $$ X $$, parameterized by the encoder.
   - $$ P(Z) $$: The prior distribution of the latent variables $$ Z $$, often chosen to be a standard normal distribution for simplicity.

2. **Role in Training**: 
   - The KL divergence between $$ Q(Z\\|X) $$ and $$ P(Z) $$ is a part of the loss function in VAEs. Specifically, it's used in the Evidence Lower Bound (ELBO) as a regularizer.
   - The ELBO maximization involves minimizing the KL divergence, which encourages the variational distribution $$ Q(Z\\|X) $$ to be close to the prior $$ P(Z) $$. This ensures that the latent space has good properties (like continuity and completeness) and prevents overfitting to the training data.

3. **Mathematical Form in VAEs**: If both $$ Q(Z\\|X) $$ and $$ P(Z) $$ are Gaussian distributions, the KL divergence has an analytical form, which makes computation efficient during training.

4. **Interpretation in VAEs**: Minimizing the KL divergence in VAEs ensures that the learned latent space representation does not deviate too far from the prior assumption, which is often desirable for generative modeling.

In summary, in the context of VAEs, the KL divergence is used to measure and minimize the difference between the learned variational distribution of the latent variables and their prior distribution. This is a key component in ensuring that VAEs learn meaningful and generalizable latent representations of the data.



<a id="appendix5"></a>

Kullback-Leibler (KL) divergence for two gaussians
--------------------------------------------------------


When $$ Q(Z\\|X) $$ for $$ X = x_1 $$ is a Gaussian distribution and the prior is also a Gaussian, the KL divergence can be calculated analytically. This is a common scenario in Variational Autoencoders (VAEs), where both the variational distribution (approximated by the encoder) and the prior distribution over the latent variables are assumed to be Gaussian. Let's break down how the KL divergence is computed in this instance:

#### Setup

1. **Variational Distribution $$ Q(Z\\|X=x_1) $$**: 
   - Assume it is a Gaussian with mean $$ \mu $$ and covariance matrix $$ \Sigma $$, which are outputs of the encoder for the data point $$ x_1 $$. In practice, for computational reasons, the covariance matrix $$ \Sigma $$ is often assumed to be diagonal, and we might use $$ \sigma^2 $$ to denote its diagonal elements.

2. **Prior Distribution $$ P(Z) $$**: 
   - Typically assumed to be a standard Gaussian distribution, $$ \mathcal{N}(0, I) $$, with mean 0 and identity covariance matrix.

#### KL Divergence Calculation

The KL divergence between two multivariate Gaussian distributions has a closed-form expression. If $$ Q(Z\\|X=x_1) = \mathcal{N}(\mu, \Sigma) $$ and $$ P(Z) = \mathcal{N}(0, I) $$, the KL divergence is given by:

$$ D_{KL}(Q(Z\\|X=x_1) || P(Z)) = \frac{1}{2} \left( \text{tr}(\Sigma) + \mu^T \mu - k - \log \det(\Sigma) \right) $$

Where:
- $$ \text{tr}(\Sigma) $$ is the trace of $$ \Sigma $$ (the sum of its diagonal elements).
- $$ \mu^T \mu $$ is the dot product of $$ \mu $$ with itself.
- $$ k $$ is the dimensionality of the latent space $$ Z $$.
- $$ \log \det(\Sigma) $$ is the natural logarithm of the determinant of $$ \Sigma $$.

#### Simplification for Diagonal $$ \Sigma $$

In many VAE implementations, $$ \Sigma $$ is diagonal, and its elements are represented as $$ \sigma^2_i $$ (the variances). The KL divergence simplifies to:

$$ D_{KL}(Q(Z\\|X=x_1) || P(Z)) = \frac{1}{2} \left( \sum_i(\sigma_i^2 + \mu_i^2 - 1 - \log \sigma_i^2) \right) $$

#### Summary

- In this scenario, where both the variational and prior distributions are Gaussian, the KL divergence can be computed directly using the parameters $$ \mu $$ and $$ \Sigma $$ provided by the encoder for a specific data point $$ x_1 $$.
- This analytical form of the KL divergence is one of the reasons Gaussian distributions are commonly used in VAEs, as it allows for efficient computation during the training process.


<a id="appendix6"></a>

Reconstruction loss
--------------------------------------------------------

The term $$ \mathbb{E}_{Q(Z\\|X)}[\log P(X\|Z)] $$ in the context of Variational Autoencoders (VAEs) represents the reconstruction loss. This term is a crucial part of the Evidence Lower Bound (ELBO) that VAEs optimize during training. Let's break down what this term means and why it's considered the reconstruction loss:

### Understanding $$ \mathbb{E}_{Q(Z\|X)}[\log P(X\|Z)] $$:

1. **$$ Q(Z\|X) $$**: This is the variational distribution of the latent variables $$ Z $$ given the observed data $$ X $$, parameterized by the encoder part of the VAE. It represents the encoder's approximation of how the latent variables could be distributed for a given input $$ X $$.

2. **$$ P(X\\|Z) $$**: This term is the likelihood of the data $$ X $$ given the latent variables $$ Z $$. It is modeled by the decoder part of the VAE, which tries to reconstruct the input data from the latent variables.

3. **Expectation $$ \mathbb{E} $$**: The expectation is taken over the variational distribution $$ Q(Z\\|X) $$. This means we're averaging over all possible values of the latent variables $$ Z $$, weighted by how probable those values are according to the variational distribution.

### Why It's Called Reconstruction Loss:

- **Role of Decoder**: The decoder's job is to take latent variables $$ Z $$ and reconstruct the original data $$ X $$. The term $$ \log P(X\\|Z) $$ measures how well the decoder is doing this job. Specifically, it's the log likelihood of the reconstructed data matching the original data.

- **Log Likelihood**: Using the logarithm of the likelihood (instead of raw likelihood) has several mathematical advantages, including numerical stability and transforming multiplication of probabilities into summation, which is easier to work with in optimizations.

- **Averaging Over Latent Space**: By taking the expectation with respect to $$ Q(Z\\|X) $$, the reconstruction loss accounts for the uncertainty in the latent space. It evaluates the decoder's performance across the range of latent representations that the encoder deems likely for the given input.

### In Practice:

- During training, the VAE adjusts the parameters of both the encoder and decoder to maximize this term. Maximizing $$ \mathbb{E}_{Q(Z\\|X)}[\log P(X\|Z)] $$ means improving the ability of the decoder to accurately reconstruct the input data from the latent variables.
- This term encourages the VAE to learn meaningful and useful representations in the latent space. If the latent variables $$ Z $$ capture the essential features of $$ X $$, the decoder will be more successful at reconstruction, leading to a higher value of this term.

### Summary

In summary, $$ \mathbb{E}_{Q(Z\\|X)}[\log P(X\|Z)] $$ in VAEs represents the reconstruction loss. It quantifies how well the model can reconstruct the original input data from its latent representation, and its optimization leads to a better learning of both the latent space and the data reconstruction process.


<a id="appendix7"></a>

Elbo (Evidence Lower Bound)
--------------------------------------------------------

The derivation of the Evidence Lower Bound (ELBO) is a fundamental aspect of Variational Autoencoders (VAEs) and other variational Bayesian methods. The ELBO provides a lower bound to the log-likelihood of the observed data, $$ \log P(X) $$, which is generally intractable to compute directly. Here's a step-by-step derivation:

#### Goal

We want to maximize the log-likelihood of our data $$ \log P(X) $$, but this is typically intractable due to the integration/summation over all possible configurations of the latent variables $$ Z $$. So, we introduce a tractable variational distribution $$ Q(Z\\|X) $$ to approximate the true but intractable posterior $$ P(Z\\|X) $$.

#### Starting with the Log-Likelihood

Consider the log-likelihood of our data:

$$ \log P(X) = \log \int_Z P(X, Z) \, dZ $$

#### Introducing the Variational Distribution

We can multiply and divide inside the integral by our variational distribution $$ Q(Z\\|X) $$ without changing the equation (since $$ \frac{Q(Z\\|X)}{Q(Z\\|X)} = 1 $$). This gives us:

$$ \log P(X) = \log \int_Z \frac{P(X, Z)}{Q(Z\\|X)} Q(Z\\|X) \, dZ $$

#### Applying Jensen's Inequality

Jensen's inequality allows us to move the logarithm inside the integral under certain conditions. It states that for a convex function $$ f $$, $$ f(\mathbb{E}[X]) \leq \mathbb{E}[f(X)] $$. Since the logarithm is a concave function (the negative of a convex function), we can apply Jensen's inequality in the reverse form:

$$ \log P(X) \geq \int_Z Q(Z\\|X) \log \frac{P(X, Z)}{Q(Z\\|X)} \, dZ $$

#### Expanding the Logarithm

Now, we expand the logarithm in the integrand:

$$ \log P(X) \geq \int_Z Q(Z\\|X) \left( \log P(X\\|Z) + \log P(Z) - \log Q(Z\\|X) \right) \, dZ $$

#### Splitting the Integral

Split the integral into two parts:

$$ \log P(X) \geq \int_Z Q(Z\\|X) \log P(X\\|Z) \, dZ + \int_Z Q(Z\\|X) \left( \log P(Z) - \log Q(Z\\|X) \right) \, dZ $$

#### Recognizing the KL Divergence and Expectation

The first term is the expected log-likelihood of the data given the latent variables, and the second term is the negative of the Kullback-Leibler (KL) divergence between $$ Q(Z\|X) $$ and $$ P(Z) $$:

$$ \log P(X) \geq \mathbb{E}_{Q(Z\\|X)}[\log P(X\|Z)] - D_{KL}(Q(Z\\|X) || P(Z)) $$

#### ELBO

This inequality is the ELBO:

$$ \text{ELBO} = \mathbb{E}_{Q(Z\\|X)}[\log P(X\|Z)] - D_{KL}(Q(Z\\|X) || P(Z)) $$

#### Summary

- The ELBO serves as a lower bound to the log-likelihood $$ \log P(X) $$.
- It consists of two parts: the expected log-likelihood (which encourages the reconstruction of the data) and the negative KL divergence (which regularizes the variational distribution to be close to the prior).
- Maximizing the ELBO effectively balances these two goals: reconstructing the data well while keeping the latent variable distribution close to the prior. This makes it a suitable objective for training VAEs.
